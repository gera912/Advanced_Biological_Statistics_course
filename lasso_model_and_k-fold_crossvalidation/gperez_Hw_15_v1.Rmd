---
title: "Homework, week 15: Make up some data."
author: "Gera"
date: "2/11/2020"
output: html_document
---

In this report we explore the dataset diabetes that is from the R package lars. Health researchers are interested in evaluating the effectiveness of lasso in producing more accurate and robust predictions. Here we explore  a “lasso” model which puts a prior on each coefficient of the linear predictor using stan. We then measure the model fit by k-fold cross validation, and compare this to a linear model by the residual mean square error. 

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
library(usmap)
library(ggplot2)
library(dplyr)
library(data.table)
library(tidyverse)
library(GGally)
library(HH)
library(MASS)
library(brms)
library(rstan)


dataset<-read.table("/Users/gerardoperez/Documents/shell/Bi610/UO_ABS/CLASS_MATERIALS/Homeworks/diabetes_data.tsv", header=T)

dataset<<-na.omit(dataset)
code_block <- "
data {
    int N; 
    int k;
    matrix[N,k] x;
    vector[N] y;
    int N_test;
    int k_test;
    matrix[N_test, k_test ] x_test;
    
}
parameters {
    real b0;
    real S;
    vector [k] beta;
    real<lower=0> eta;
}
model {
    S ~ exponential(1);
    beta ~ double_exponential(0,S);
    y ~ normal(b0 +x*beta, eta);
    eta ~ normal(0,10);
}      
generated quantities {
    vector[k_test] y_test;
    for (i in 1:k_test) {
      y_test[i] = normal_rng(b0 + x_test[i]*beta, eta);
    }
}
"




double_fit<- stan_model(model_code=code_block)   




```

From reading in the dataset in Rstudio we can see the that the dataset has 442 observations and 65 variables.  The variables are pre-scaled by having mean subtracted and the SD divided. To get a better visual of this data we focused on 11 observations. We let the response variable be the measure of diabetes disease progression and the other 10 variables to be the interactions. In Figure 1, we show scatterplots for each variable-combination.

**Figure 1: Pairs plot**

```{r, echo=FALSE}

x<-dataset[,2:11]
pairs(cbind(x, dataset$y))

```

From the Figure 1 we can see that tc (total cholesterol) and ldl (low density lipoprotein) have the most linear correlation. Which makes sense since ldl makes up of your body's cholesterol. 

We then went forward and built a model using stan. We used a likelihood based and well-known probability distribution,  a normal distribution for our model. Since we are trying to do a “lasso” model,  we  used double exponential (Laplace) distribution as our coefficients and set them as our betas. We multiplied these betas to our predictors x in our model. We let our standard deviation be eta and added a weakly informative priors normal(0, 10). Finally we added an baseline intercept b0 to the model. 

model {

    S ~ exponential(1);
    
    beta ~ double_exponential(0,S);
    
    y ~ normal(b0 +x*beta, eta);
    
    eta ~ normal(0,10);
}


To do a fit and predict with Stan, we used the option of adding a generated quantities block to our stan block. This allowed us to make predictions from our variable predictors using in the same program that we used to estimate the relationship between variable training data and y response training data. 


```{r, echo=FALSE,cache=TRUE, message=FALSE, warning=FALSE, include=FALSE}
K<-5
N <- nrow(dataset)
Kfold <- sample(rep(1:K, N/K))
fitted <- sampling(double_fit, 
                  data=list(N=nrow(subset(dataset, Kfold != K)), 
                  k=ncol(subset(dataset, Kfold != K))-1,
                  y=subset(dataset, Kfold != K)$y,
                  x=as.matrix(subset(dataset, Kfold != K)[,-1]),
                  N_test=nrow(subset(dataset, Kfold == K)),
                  k_test=ncol(subset(dataset, Kfold == K))-1,
                  x_test=as.matrix(subset(dataset, Kfold == K)[,-1])),
                  iter=5000, chains=3,
                  control=list(max_treedepth=12))



```


We first ran our model using random subsetting 80% to train the model and then used the remaing 20% to test our model.  We did 5000 iterations and 3 chains. From running the model, we used a  coefficient plot to visualize the credible intervals as shown in Figure 2. 

**Figure 2: 95% credible intervals**

```{r, fig.width=12, fig.height=12, echo=FALSE, message=FALSE, warning=FALSE}

stan_plot(fitted,pars=c("b0", "S", "beta", "eta" ))



```

We can see that beta 3 (bmi), beta 4 (map), and beta 9 (ltg) show to be the only positive effects with a credible interval that does not contain 0, meaning that there’s a statistically significant positive relationship to diabetes disease progression in this sampling run.


```{r, cache=TRUE, echo=FALSE, warning=FALSE, include=FALSE}

kfold_funct_2 <- function(K, dataset){
  no_na <- na.omit(dataset)
  N <- nrow(dataset)
  Kfold <- sample(rep(1:K, N/K))
  test_error <- vector()
  train_error <- vector()
  counter <- 0
  while(counter<K){
    fit <- sampling(double_fit, 
                  data=list(N=nrow(subset(dataset, Kfold != K)), 
                  k=ncol(subset(dataset, Kfold != K))-1,
                  y=subset(dataset, Kfold != K)$y,
                  x=as.matrix(subset(dataset, Kfold != K)[,-1]),
                  N_test=nrow(subset(dataset, Kfold == K)),
                  k_test=ncol(subset(dataset, Kfold == K))-1,
                  x_test=as.matrix(subset(dataset, Kfold == K)[,-1])),
                  iter=5000, chains=3,
                  control=list(max_treedepth=12))
    y_test_pred <- as.data.frame(summary(fit,pars='y_test')$summary)$mean
    test_val <- sqrt(mean((y_test_pred - subset(dataset, Kfold == K)$y)^2))
    train_val <- sqrt(mean((y_test_pred - subset(dataset, Kfold != K)$y)^2))
    test_error <- append(test_error,test_val)
    train_error<- append(train_error,train_val)
    Test<-subset(dataset, Kfold == K)$y
    Predicted<-y_test_pred
    Training<-subset(dataset, Kfold != K)$y
    counter <- counter + 1
    
  }
 c_p<-cbind(train_error,test_error)
 return( c_p)
}

output <- kfold_funct_2(5,dataset)


```

We then did k-fold crossvalidation using the same settings, same proportions of substetting the training data and test data as our previous run. Here we did k-fold crossvalidation of 5 k-folds using our model. Table 1 shows the residual mean square errors of both test set and training set. 

**Table 1: Residual mean square errors for Stan model**
```{r, echo=FALSE}

stan_test<-output[,1]
stan_train<-output[,2]
k_fold<-c(1,2,3,4,5)
stan.df<-cbind(stan_test,stan_train,k_fold)
(stan.df)


```
Here we can see that the values are lower in our training set than our test set, which implies that we are not overfitting our model.  Ideally we should have done a trace plot and coefficient plot to see for each k-fold which variables are significant. Unfortunately we did not do that here. 

In addition, to compare our model, we also ran a linear model using k-fold crossvalidation of 5 k-folds with the same proportions of substetting the training data set and test dataset as to our stan model.  Table 2 shows the results.

**Table 2: Residual mean square errors for general linear model**
```{r, echo=FALSE}

kfold <- function (K, df) {
    no_na <- na.omit(df)
    N <- nrow(df)
    Kfold <- sample(rep(1:K, N/K))

    do_xval <- function (k) {
        the_lm <- lm(y ~ ., data=df, subset=(Kfold != k))
        train_error <- sqrt(mean(resid(the_lm)^2))
        test_y <- df$y[Kfold == k]
        test_error <- sqrt(mean( (test_y - predict(the_lm, newdata=subset(df, Kfold==k)))^2 ))
        return(c('test'=test_error, 'train'=train_error))
    }

    results <- sapply(1:K, do_xval)
    return(results)
}

t<-kfold(5, dataset)
lm_test<-as.vector(t[1,])
lm_train<-as.vector(t[2,])

lm.df<-cbind(lm_test,lm_train, k_fold)
(lm.df)




```
Here we see the similar trend as to our stan model, residual mean square errors are lower in our training set than the test set which impies we are not overfitting. 

Finally we compare the average residual mean square errors for both models with the training set and test set. Table 3 shows the results. 

**Table 3: Linear model vs Stan model by average residual mean square errors**
```{r, echo=FALSE}

lm_test<-mean(lm_test)
lm_train<-mean(lm_train)
stan_test<-mean(stan_test)
stan_train<-mean(stan_train)
final<-cbind(lm_test, lm_train, stan_test, stan_train)
rownames(final)<-"mean"
(final)
```

Our values in our Stan model specifically our test set is higher than the linear model values. This indicates that our stan model is a poor model compared to the linear model. Most likely there is something wrong with our model since we expected to show that our Stan model would be more accurate and robust. 